{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0370126f-8bd6-4bef-b83b-77017391fb95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42224840-9fc4-4ea8-8a8d-3ff4003b46a1",
   "metadata": {},
   "source": [
    "# Load and Process Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e849c4-ea50-4e1d-bcd9-947ebc16cdaa",
   "metadata": {},
   "source": [
    "## Function to load and preprocess the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21df3b8f-f795-4941-a638-a898466fd44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image(image_path):\n",
    "    img = PIL.Image.open(image_path)\n",
    "    img = img.resize((400, 400))  # Resize for consistency\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)  # Preprocess for VGG19\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a1df0-69b9-4648-bc0c-9042b8e68d72",
   "metadata": {},
   "source": [
    "## Function to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f0e45a-5124-4eb8-8f04-76e67c636a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=\"Image\"):\n",
    "    image = image[0]  # Remove batch dimension\n",
    "    image = np.clip(image, 0, 255).astype('uint8')  # Clip to valid range\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c5a13-0caf-4fe0-bce2-e3ba84fbc431",
   "metadata": {},
   "source": [
    "# Define Content and Style Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ba884-b821-4fd5-98e0-bef898b60863",
   "metadata": {},
   "source": [
    "## Function to compute content loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46dd71b9-58cc-4c2d-8bc4-007a485aa424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base_content, target):\n",
    "    return tf.reduce_mean(tf.square(base_content - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1c24-dda3-43f1-83ba-7bd72dbbf137",
   "metadata": {},
   "source": [
    "## Function to compute style loss using Gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0a69fc-f40c-45c7-b3e9-147efc97d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    channels = int(tensor.shape[-1])\n",
    "    vectorized = tf.reshape(tensor, [-1, channels])\n",
    "    gram = tf.matmul(tf.transpose(vectorized), vectorized)\n",
    "    return gram / tf.cast(tf.size(vectorized), tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285628a-b4b3-48a0-9073-d7e3b38fa33e",
   "metadata": {},
   "source": [
    "## Compute style loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db739c57-cf27-4237-a0c6-0df08f1958ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(base_style, gram_target):\n",
    "    gram_base = gram_matrix(base_style)\n",
    "    return tf.reduce_mean(tf.square(gram_base - gram_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160574f5-7abd-4c45-a11c-6c0a78cee0ea",
   "metadata": {},
   "source": [
    "# Load Pre-Trained VGG19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96c0903-1cc9-4a1d-b465-963005bbae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG19 model without the fully connected layers\n",
    "def get_model():\n",
    "    vgg = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "    vgg.trainable = False  # Freeze the model\n",
    "    \n",
    "    # Layers used for content and style representation\n",
    "    content_layers = ['block5_conv2']\n",
    "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "    \n",
    "    outputs = {layer.name: vgg.get_layer(layer.name).output for layer in (style_layers + content_layers)}\n",
    "    \n",
    "    return Model(inputs=vgg.input, outputs=outputs), style_layers, content_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1476f-1dab-4bee-b5c8-0ace3ad60d45",
   "metadata": {},
   "source": [
    "# Define Optimization and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34224827-205c-4ffc-b639-f54f8fe437e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from model\n",
    "def get_features(image, model, style_layers, content_layers):\n",
    "    outputs = model(image)\n",
    "    style_features = [outputs[layer] for layer in style_layers]\n",
    "    content_features = [outputs[layer] for layer in content_layers]\n",
    "    return style_features, content_features\n",
    "\n",
    "# Training function\n",
    "def train(content_path, style_path, epochs=500, alpha=1e4, beta=1e-2):\n",
    "    # Load images\n",
    "    content_image = load_and_process_image(content_path)\n",
    "    style_image = load_and_process_image(style_path)\n",
    "\n",
    "    # Load VGG model\n",
    "    model, style_layers, content_layers = get_model()\n",
    "    \n",
    "    # Extract features\n",
    "    style_features, _ = get_features(style_image, model, style_layers, content_layers)\n",
    "    _, content_features = get_features(content_image, model, style_layers, content_layers)\n",
    "    \n",
    "    # Initialize generated image\n",
    "    generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5.0)\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            gen_style_features, gen_content_features = get_features(generated_image, model, style_layers, content_layers)\n",
    "\n",
    "            # Compute total loss\n",
    "            content_loss_value = content_loss(gen_content_features[0], content_features[0])\n",
    "            style_loss_value = sum(style_loss(gen, target) for gen, target in zip(gen_style_features, style_features))\n",
    "            total_loss = alpha * content_loss_value + beta * style_loss_value\n",
    "\n",
    "        # Compute gradients and update generated image\n",
    "        grad = tape.gradient(total_loss, generated_image)\n",
    "        optimizer.apply_gradients([(grad, generated_image)])\n",
    "        generated_image.assign(tf.clip_by_value(generated_image, -128, 127))\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}/{epochs} - Loss: {total_loss.numpy()}\")\n",
    "\n",
    "    return generated_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f723d-bfb6-4cd8-b9a8-878d32add0e5",
   "metadata": {},
   "source": [
    "# Run the Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a964bcc-a26a-40f6-9b7f-60a8692fe2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 0us/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m style_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpexels-kuremo-8660213.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m stylized_image \u001b[38;5;241m=\u001b[39m train(content_path, style_path)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Display the final output\u001b[39;00m\n\u001b[0;32m      9\u001b[0m show_image(stylized_image\u001b[38;5;241m.\u001b[39mnumpy(), title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStylized Image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(content_path, style_path, epochs, alpha, beta)\u001b[0m\n\u001b[0;32m     12\u001b[0m style_image \u001b[38;5;241m=\u001b[39m load_and_process_image(style_path)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load VGG model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model, style_layers, content_layers \u001b[38;5;241m=\u001b[39m get_model()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m     18\u001b[0m style_features, _ \u001b[38;5;241m=\u001b[39m get_features(style_image, model, style_layers, content_layers)\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m content_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock5_conv2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m style_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock1_conv1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock2_conv1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock3_conv1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock4_conv1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock5_conv1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {layer\u001b[38;5;241m.\u001b[39mname: vgg\u001b[38;5;241m.\u001b[39mget_layer(layer\u001b[38;5;241m.\u001b[39mname)\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m (style_layers \u001b[38;5;241m+\u001b[39m content_layers)}\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Model(inputs\u001b[38;5;241m=\u001b[39mvgg\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39moutputs), style_layers, content_layers\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# Provide paths to your images\n",
    "content_path = 'tsuyoshi-kozu-VuSiUScxre0-unsplash.jpg'\n",
    "style_path = 'pexels-kuremo-8660213.jpg'\n",
    "\n",
    "# Run the training\n",
    "stylized_image = train(content_path, style_path)\n",
    "\n",
    "# Display the final output\n",
    "show_image(stylized_image.numpy(), title=\"Stylized Image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8433af0-621a-4fdc-9a7d-813381d95e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
